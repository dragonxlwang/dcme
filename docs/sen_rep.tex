\documentclass{article} % For LaTeX2e
\usepackage{nips13submit_e,times}
\usepackage{hyperref}
\usepackage{url}
%\documentstyle[nips13submit_09,times,art10]{article} % For LaTeX 2.09
\definecolor{dgreen}{rgb}{0.00,0.39,0.00}

\title{Sentence Level Representation}


%\author{
%David S.~Hippocampus\thanks{ Use footnote for providing further information
%about author (webpage, alternative address)---\emph{not} for acknowledging
%funding agencies.} \\
%Department of Computer Science\\
%Cranberry-Lemon University\\
%Pittsburgh, PA 15213 \\
%\texttt{hippo@cs.cranberry-lemon.edu} \\
%\And
%Coauthor \\
%Affiliation \\
%Address \\
%\texttt{email} \\
%\AND
%Coauthor \\
%Affiliation \\
%Address \\
%\texttt{email} \\
%\And
%Coauthor \\
%Affiliation \\
%Address \\
%\texttt{email} \\
%\And
%Coauthor \\
%Affiliation \\
%Address \\
%\texttt{email} \\
%(if needed)\\
%}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy% Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
This document reports the current state-of-the-art study on sentence level
representation of language/text and our work on this task with application
to multilingual translation at Facebook.
\end{abstract}

\section{What is a sentence?}
Linguistically, a sentence is one unit in the hierarchy of language --- It sits
above words and phrases, while below paragraphs (disclosures), documents, and
corpus. Usually, a sentence conveys relatively complete semantics by a stream of
words, the order of which is decided by the syntax, or commonly referred by
people as the grammar.

\section{How is a sentence generated?}
For example, ``I work at Facebook'' is a sentence. In this section, we consider
the problem of sentence generation.
Cognitively, a sentence can be viewed as generated by syntax with the operands
of each syntactic rule being the lower level units in the language hierarchy.
Till now, most of linguistics adopted two version of language hierarchy: 1)
a sentence is generated as a sequence of ordered words; 2) a sentence is a
syntactical complete phrase, where a phrase is generated as a tree with
children being either phrases, or words. The following two sets of formulas
depicts the difference:

\begin{itemize}
\item Sequence:
	\begin{eqnarray}
	\textrm{smn}(sent) &=& \textrm{syn}(\textrm{smn}(``I~work~at''),
\textrm{smn}(``Facebook'')) \\
	\textrm{smn}(``I~work~at'') &=& \textrm{syn}(\textrm{smn}(``I~work''),
\textrm{smn}(``at''))	\nonumber \\
	\textrm{smn}(``I~work'') &=& \textrm{syn}(\textrm{smn}(``I''),
\textrm{smn}(``work''))	\nonumber
	\end{eqnarray}
\item Tree:
	\begin{eqnarray}
	\textrm{smn}(sent) &=& \textrm{syn}(\textrm{smn}(``I''),
\textrm{smn}(``work~at~Facebook'')) \\
	\textrm{smn}(``work~at~Facebook'') &=& \textrm{syn}(\textrm{smn}(``work''),
\textrm{smn}(``at~Facebook''))	\nonumber \\
	\textrm{smn}(``at~Facebook'') &=& \textrm{syn}(\textrm{smn}(``at''),
\textrm{smn}(``Facebook''))	\nonumber
	\end{eqnarray}
\end{itemize}

It is easy to see that both schemes are essentially recurring. Especially, the
sequence version is a special case of the tree version in that the syntactic
composition is restricted to take only the immediate next (right) token. Thus,
in terms of power of representation, the tree version had an advantage of being
more flexible. However, its downside of involving the sentence parsing makes it
inefficient for longer (and complex) sentences or less grammatically
well-defined sentences (such as posts, twitters, etc) as the decoding takes
longer running time and the confidence of parsing becomes lower.

\section{Sentence representation as a vector}
For the purpose of learning tasks such as classification, regression, etc., it
is desirable to have a fixed dimension representation for sentences. That is to
say, for the representation task, we need to project a sentence
``$w_1, \dots, w_n$'' onto a N-dimension space as a point $x = [x_1, \dots,
x_N]$ where $N$ is usually a predefined task-specific parameter. Representation
of language in this way is generally called as distributed representation
because each dimension is believed to be semantically independent, and one
sentence is a set of weights distributed across N dimensions.

Ideally, a good representation approach should have ``grouping property'':
(semantically) similar words/phrases/sentences should be geometrically close
to each other. For example, synonymous words shall be grouped; sentences with
same meaning but changed into passive voice also should be close to its active
voice form. Besides of the ``grouping property'', in some other works, other
additional geometric properties~(regularity) are evaluated for distributed
representation (on words):
$\textrm{smn}(``biggest'') - \textrm{smn}(``big'')$ and
$\textrm{smn}(``smallest'') - \textrm{smn}(``small'')$ should be
close~(\cite{mikolov2013efficient,mikolov2013distributed});
$\textrm{smn}(``king'') - \textrm{smn}(``man'')$ and $\textrm{smn}(``queen'') -
\textrm{smn}(``woman'')$ should be close\cite{mikolov2013linguistic}.


\section{Related work}
As aforementioned, there are majorly two ways of modeling sentences:
as a sequence or as a tree. Most related works falls in either one
of them.

\subsection{Word Embedding}
According to \cite{baroni2014don} which compares context-counting vs.
context-predicting models of distributed semantic word embedding approaches, it has
been witnessed that predicting models (language model-based) is generally
superior across different applications. We review some of the both methods in
below.

\cite{pennington2014glove}:
a counting based method which factorize the log cooccurrence matrix with
	weighted penalty. It also establish some comparison with skip-gram and
	inverse vLBL.

\cite{bengio2003neural}, \cite{morin2005hierarchical},
	\cite{mnih2007three}, \cite{mnih2009scalable}, \cite{mnih2012fast},
	\cite{mnih2013learning}, : Language model-based with fixed history.
\begin{itemize}
\item Model:
	\begin{itemize}
	\item neural network (hyperbolic tangent $\mathrm{tanh}$ and softmax).
	Input is the history computed as affine function of previous $t$ words.
	Prediction is on the next word using Softmax, which is notorious because of
	its complexity of the partition function. \cite{bengio2003neural}
	\item RBM and log-bilinear: RBM involves hidden layer and words (previous
	and current words); log-bilinear computes quadric form of previous words
	and current word. \cite{mnih2007three}
	\end{itemize}
\item Solution to Softmax complexity:
	Hierarchical versions are implemented as arranging words in a tree coding
	and compute predicted word distribution as a series of branching decision.
	\begin{itemize}
	\item \cite{morin2005hierarchical}: hierarchy is built by WordNet IS-A
			taxonomy and k-means converting to binary tree.
	\item \cite{mnih2009scalable}: log-bilinear with Hierarchy built by current
		embedding clustering via binary mixture Gaussian and updated while
		iterations.
	\end{itemize}
	Noise-Contrastive Estimation: In certain condition, the maximal likelihood
	for Softmax can be obtained if we classify current true word vs. random
	words. \cite{mnih2012fast},\cite{mnih2013learning}
\item Training criteria: maximal likelihood of prediction of next words given
    previous $t$ words.
\end{itemize}


\cite{collobert2008unified}:
\begin{itemize}
\item Model: Convolution neural network (convolution layer, max pooling,
             $\mathrm{tanh}$ and softmax)
\item Training criteria: multitask learning with shared word embedding and
    task-specific prediction prediction model. For Language model, it is
    trained on hinge loss of predicting correct word vs. random words. vFor
    SRL, additional features (relative positive w.r.t. the predicate) are
    used.
\item Learning: stochastic looping over tasks and SGD. Tasks include SRL, POS,
    Chunking, NER, Synonyms and Language model.
\end{itemize}

\cite{mikolov2013efficient}, \cite{mikolov2013distributed}:
\begin{itemize}
\item Model: CBow (using context words (previous and next) to predict middle
    one) and Skip-gram (predicting context words using current word).
   	\begin{itemize}
   	\item CBow can be seen as composition method where composition is done by
   	average.
   	\end{itemize}
\item Training criteria: maximal likelihood.
\item Learning:
	\begin{itemize}
	\item Hierarchical softmax: vocabulary encoded by Huffman tree.
	\item Negative sampling: simplified (in the same spirit but lacks theoretic
			soundness) Noise-Contrastive Estimation.
	\end{itemize}
\item Phrase: phrase embedding is learned once they are identified by other
	methods and then treated as single words.
\item Additive compositionality: ``Czech'' + ``currency'' $\approx$ ``koruna''
\item Word offset: ``king'' - ``man'' $\approx$ ``queen'' - ``woman''
\end{itemize}

\subsection{Semantic Composition}

\subsubsection{Direct learning}
\cite{yin2014exploration}: target unit
\begin{itemize}
	\item Phrase collection: extracted using Wiktionary,
		Wordnet as continuous or discontinuous.
	\item phrase embedding: learned as single token; no
		composition;
evalution: animacy
	\item classification; paraphrase detectione (MSRPC, sentence composition
	by addition over all words and phrases);
\end{itemize}


\cite{zhao2015phrase}: regression on phrase level representation
\begin{itemize}
\item Phrase collection and embedding: using wikipedia and skip-gram
\item regression by syntactical typed (low-rank) tensor
\end{itemize}

\subsubsection{Operator based Composition}
\cite{guevara2010regression}, \cite{baroni2010nouns}, \cite{grefenstette2011experimental},
\cite{erk2008structured}:
Syntactic typed composition:
\begin{itemize}
\item \cite{guevara2010regression}:
	\begin{itemize}
	\item space: adj. noun - vector
	\item composition:
	``adj noun'' = $W_a$ $\cdot$ adj + $W_n$ $\cdot$ noun
	\item parameters: supervised learned by regression;
		gold phrase embedding learned as single token
	\end{itemize}
\item \cite{baroni2010nouns}:
	\begin{itemize}
	\item space: adj. - matrix, noun - vector
	\item composition: ``adj noun'' = adj $\cdot$ noun
	\item parameters: supervised learned by regression;
		gold phrase 	embedding learned as single token
	\end{itemize}
\item \cite{grefenstette2011experimental}:
	\begin{itemize}
	\item space: verb - matrix; adj, adv, n. - vector;
	\item composition: ``adj. sub verb obj adv'' =
	(adj $\odot$ verb)
	$\odot$ ( (adj $\odot$ sub) $\otimes$ obj)
	\item parameters: counting-based; no learning
	\end{itemize}
\item \cite{erk2008structured}
	\begin{itemize}
	\item space: word encoded as $w = (v, R, R^{-1})$
		\begin{itemize}
		\item $w$: semantic meaning of the word
		\item $R, R^{-1}$: modifying vector per relation
			(directed selectional prefernce)
		\end{itemize}
	\item composition: $a \stackrel{r}{\rightarrow} b$,
		contextual embedding for $a$ becomes
		\begin{equation}
		(v_a \odot R_b^{-1}(r), R_a - {r}, R_a^{-1})
		\end{equation}
	\item parameters: count-based word embedding and
		selectional preference. No optimization required.
		Essentially, this becomes (syntactic) typed $\odot$
		composition
	\item evaluation: subject-verb and landmark similarity
		prediction~\cite{mitchell2008vector}; paraphrases
		ranking. (ranking substitute for contextual word)
	\end{itemize}
\end{itemize}

\cite{rudolph2010compositional}, \cite{yessenalina2011compositional}: words as matrices
\begin{itemize}
\item Problem: phrase level representation
\item Matrix-space model: words are (``simple enough'') matrices and
	composition is matrix multiplication. This can recover vector composition
	such as addition, multiplication, and Holographic reduce representation~
	\cite{plate1991holographic}
\item Evaluation: sentiment analysis~\cite{yessenalina2011compositional};
\end{itemize}

\cite{mitchell2008vector}, \cite{blacoe2012comparison}:
\begin{itemize}
\item Problem: phrases/sentences semantic composition
\item \cite{mitchell2008vector}:
	\begin{itemize}
	\item word embedding: counting based
	\item composition: additive, kintsch, multiplicative~($\odot$), tensor
		product, convolution, linear combination, dilation, headed only, target
		unit
	\end{itemize}
\item Methods:
	\begin{itemize}
	\item Word embedding: counting (lexical and syntactic) and prediction-based
	\item Composition: additive, multiplication and recursive autoencoder (REA,
			parse tree required).
	\end{itemize}
\item Evaluation: phrase similarity (Mitchell and Lapata (2010)~); paraphrase
	detection (Microsoft Research Paraphrase Corpus (MSRPC)~)
\end{itemize}

\cite{mitchell2009language}: Composition-based language model
\begin{itemize}
\item Problem: language model
\item Method: additive or multiplicative composition to compute history for
	prediction on next work; no learning on embedding or composition model
	while language modeling.
\end{itemize}

\subsubsection{Neural Network based}
\cite{kalchbrenner2014convolutional}, \cite{hu2014convolutional},
\cite{kim2014convolutional}: CNN
\begin{itemize}
\item Problem: task-specific sentence level modeling
\item Methods: CNN (convolution with (dynamic) max pooling), and top
	layer task-specific softmax classier/regression/perceptron
\item Tasks:
	\begin{itemize}
	\item \cite{kalchbrenner2014convolutional}: sentiment analysis
		(binary, multitude); question-type classification;
	\item \cite{hu2014convolutional}: sentence completion/matching;
		paraphrase detection (MSRPC)
	\item \cite{kim2014convolutional}: sentence-level classification
		tasks. (SA, question-type, etc)
	\end{itemize}
\end{itemize}

\cite{mikolov2010recurrent}, \cite{mikolov2012context},\cite{mikolov2013linguistic},
\cite{auli2013joint},\cite{sutskever2014sequence}: RNN (sequence)
\begin{itemize}
\item Model: Recurrent neural network (sigmoid + softmax) based
	language/translation model.
	It accommodates unbounded length of history for prediction. History
	can be viewed as composition until current words.
	\cite{mikolov2010recurrent}, \cite{mikolov2013linguistic} and learns
		LM using sequential RNN. \cite{mikolov2012context} adds input of
		context (LDA context-topic vector). \cite{auli2013joint} jointly
		learns translation (RNN on target language) with additional
		input of source sentence	vector (LDA or counting) and translate
		via lattice rescoring.
	\cite{sutskever2014sequence} (LSTM-RNN) learns translation only
		(decoding by beam search; rescoring base system).
\item Training criteria: maximal likelihood on predicting next words.
\item Embedding properties: word relations computed by \textit{word offset}.
    $w_a:w_b = w_c:w_d$, then $v_a - v_b \approx v_c - v_d$
    \cite{mikolov2010recurrent}, \cite{mikolov2013linguistic}
	\begin{itemize}
	\item Syntactic regularity: Base/Comparative/Superlative, Singular/Plural
	    Non-possessive/Possessive, Base/Past, etc.
	\item Semantic retularity: e.g. Class-Inclusion:Singular Collective
	    (clothing:shirt, dish:bowl).
	\end{itemize}
\end{itemize}

\cite{bordes2012joint}: RNN for fixed structure.
\begin{itemize}
\item Problem: composition for (subject, verb, object) tuples. Strictly,
	this work does not yield composition but can be easily modified to do so.
\item Training criteria: maximal margin against random perturbed samples.
\end{itemize}


\cite{socher2010learning}, \cite{socher2011dynamic}, \cite{socher2012semantic},
\cite{socher2013parsing}: RNN
\begin{itemize}
\item \cite{socher2010learning}: parsing tree-guided composition
	\begin{itemize}
	\item composition structure decoding model: the combination type is learned
		as a softmax classification; the strength of the combination is a score
		linear in their composition vector.
	\item model: word embedding; composition categorical/strength (scoring)
		model; composition model
	\item learning: minimization structured hinge loss of labeled parsing trees
	\item evaluation: parsing
	\end{itemize}
\item \cite{socher2011dynamic}: parse tree-guided REA composition
	\begin{itemize}
	\item composition structure: given by constituent parsing tree;
	\item learning: reconstruction error minimization
	\item model: composition model
	\item evaluation: paraphrase detection (MSRPC); features are extracted
		from embeddings of all nodes of pair sentences using pooling and
		classified by softmax (logistic regression).
	\end{itemize}
\item \cite{socher2012semantic}: word/phrase is a (matrix, vector) pair
	\begin{itemize}
	\item composition structure: given by constituent parsing tree;
	\item matrix vector - RNN:  composite (A, a), (B, b) as
		$p = g(W_1 \cdot Ba + W_2 \cdot Ab)$; low rank matrix approximation
	\item learning: task-specific supervise training by stack softmax
		classification
	\item evaluation: sentiment treebank
	\end{itemize}
\item \cite{socher2013parsing}: parsing tree-guided composition
	\begin{itemize}
	\item words/phrase representation: vector + (pos) tag
	\item compositional vector grammer:
		 \begin{itemize}
		 \item vector: typed composition squashed affine in
		 	children vector
		 \item parent tag: computed by PCFG
		 \item compositional strength: addition of vector
		 	composition strength and PCFG
		 \end{itemize}
	\item learning: minimization structured hinge loss of
		labeled parsing trees
	\item model: syntactically typed vector composition
		composition strength (scoring) model.
	\item evaluation: parsing
	\end{itemize}
\end{itemize}


\cite{irsoy2014deep}, \cite{le2015compositional}:
	variant RNN (parse tree)
\begin{itemize}
\item Problem: task-specific sentence level modeling
\item Methods:
	\begin{itemize}
	\item \cite{irsoy2014deep}: multilayer-RNN (deep-RNN). More layers provide
		higher abstraction with different embedding dimensions.
	\item \cite{le2015compositional}: LSTM-RNN
	\end{itemize}
\item Evaluation: Sentiment classification (sentiment treebank)
\end{itemize}







\subsection{Semantic Composition}
\begin{eqnarray}
T({\color{dgreen} V_1}, {\color{dgreen} V_2}) =  \sum\limits_{i}^{M}\sum\limits_{i}^{M}
({\color{dgreen} V_1} \odot {\color{dgreen} V_2})_{i,j}~ \cdot {\color{blue} a_{i,j} } \odot
{\color{blue} b_{i,j} } \odot  {\color{blue} c_{i,j}} \nonumber \\
{\color{red} S} = T({\color{dgreen} V_1}, {\color{dgreen} V_2})
\times_1 ({\color{red} S_1}; 1) \times_2 ({\color{red} S_2}; 1) \nonumber \\
V = \sigma\left( T^{SYN} \times_1 {\color{dgreen} V_1} \times_2 {\color{dgreen} V_2} \right) \nonumber \\
\mathcal{L}(t) = \max_{t'} \left(score\left(t'\right) - score\left(t\right) + \Delta(t, t')
\right)
\end{eqnarray}

\begin{eqnarray}
T = \sum\limits_{i} \sum\limits_{j} p_{i, j} T_{i,j} \\
p_{i,j} \geq 0, \sum\limits_{i,j} p_{i,j} = 1 ~\textrm{(simplex)} \\
p_{i,j} = p_{i}^{v_1} p_{j}^{v_2} \\
p_i^{v} \propto \exp(v_i) \\
T_{i,j} = \sum\limits_k {\color{blue} a_{i,j}^{(k)} } \odot {\color{blue} b_{i,j}^{(k)} } \odot  {\color{blue} c_{i,j}^{(k)}} \\
{w'}_x &=& \sum\limits_{i,j} p_{i,j} M_{i,j}^{left} {\color{red}w_z} \\
{w'}_y &=& \sum\limits_{i,j} p_{i,j} M_{i,j}^{right} {\color{red}w_z} \\
\mathrm{score}(x,y,z) = -\frac{1}{2}( || w_x - {w'}_x ||^2  + ||w_y - {w'}_y||^2)  \\
\mathrm{score}(tree) =  \mathrm{score}(root) + \rho \cdot (\mathrm{score}(left\_sub\_tree) + Score(right\_sub\_tree) )

\end{eqnarray}

\iffalse
\subsection{Sentence as a Sequence}
This line of research models sentences as sequences. Early works include
using sequential RNN~\cite{mikolov2010recurrent, mikolov2012context} (as
language model prediction) and LSTM~\cite{sutskever2014sequence}.
LSTM improves RNN by adding self-recurrent connections with fixed weight
1.0 which ensuring the representation remains constant barring any outside
interference.

\subsection{Sentence as a Tree}
\subsection{Hybrid}
\subsection{Sentence Representation and Translation}
In a representation-alone task, the training objective is usually defined by
self-translation or prediction -based metrics. Self-translation aims at provide
a good auto-encoding/decoding ability where the learned vector representation
from sentence~(encoding) should also be able to mapped back to
sentence~(decoding)~\cite{socher2011dynamic}. Prediction-based approaches
usually adopts the sequence modeling of sentences, and requires that the
intermediate representation of part of sentences yields good prediction power
of the next work, which is similar to the idea of language
modeling~\cite{mikolov2013distributed}.

In a representation-X task (where X could be sentiment analysis, document
classification, etc), additional supervision is available for guiding the
training process. By stacking the regression/classification algorithm on top of
the representation network, the learned representation can be simply fine tuned
via the additional labels for specific tasks.

In a representation-translation task, similar to using self-translation, one can
also take the mapping from source language sentence representation to target
language as a decoding process~\cite{sutskever2014sequence,}. In those
approaches, since the decoding (translation from representation vector to target
language) is independent of the source language once the representation vector
is given, the sentence representation is more language-independent than those
learned by representation-alone (or -X) approaches. In addition, one can also
build the representation on prediction task.

\fi


\subsubsection{References}
\bibliographystyle{apalike}
% \bibliography{/Users/xlwang/Dropbox/Reading/refs.bib}
\bibliography{refs.bib}
\end{document}
